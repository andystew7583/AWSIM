%%%
%%% createRunScript.m
%%%
%%% Convenience function to create an execution script for model runs.
%%%
%%% local_home_dir      Base directory path to hold experiment folder
%%% run_name            Name of experiment folder
%%% model_code_dir      Path to model source code files (on the machine on
%%%                     which the code will be compiled and executed)
%%% exec_name           Name of the compiled code executable
%%% use_intel           Set true to compile using intel compilers, false
%%%                     for gcc
%%% use_pbs             Set true to use grid engine to run the job on a
%%%                     remote cluster, false for regular execution
%%% use_cluster         Set true to create scripts to upload/download the
%%%                     experiment to/from a remote computing cluster
%%% uname               Username to log into the remote computing cluster
%%% cluster_addr        DNS address of the remote computing cluster
%%%                     Currently two options are supported as examples:
%%%                     1. caolila.atmos.ucla.edu
%%%                     2. hoffman2.idre.ucla.edu
%%%                     Add your own!
%%% cluster_home_dir    Base directory path to hold experiment folder on 
%%%                     the remote computing cluster     
%%%
function createRunScript (  local_home_dir, ...
                            run_name, ...
                            model_code_dir, ...
                            exec_name, ...
                            use_intel, ...
                            use_pbs, ... 
                            use_cluster, ...
                            uname, ...
                            cluster_addr, ...
                            cluster_home_dir)

  %%% File/directory names    
  local_run_dir = fullfile(local_home_dir,run_name);    
  sfname = fullfile(local_run_dir,'Run.sh');
  cleanfname = fullfile(local_run_dir,'Clean.sh');
  upfname = fullfile(local_run_dir,'Upload.sh');
  downfname = fullfile(local_run_dir,'Download.sh');  
  
  %%% Create a script file to run the code
  sfid = fopen(sfname,'w');
  if (sfid == -1)
    error(['Could not open ',sfname]);
  end

  %%% Copy code files over
  if (~copyfile(fullfile(model_code_dir,'/*'),local_run_dir))
    error(['Could not copy code files to ',local_run_dir]);
  end

  %%% To use intel compilers, need to add executables and libraries to system path
  if (use_intel)
    fprintf(sfid,'source /opt/intel/bin/compilervars.sh intel64\n');
  end

  %%% This line executes the C code
  run_str = ['./',exec_name,' ',run_name,'_in ','. \n'];    
  
  %%% PBS submission requires a PBS script to be prepared
  if (use_pbs)
    
    pbsfid = fopen(fullfile(local_run_dir,'run_pbs'),'w');
    
    switch (cluster_addr)
      
      case 'caolila.atmos.ucla.edu'
      
        pbsstr = ...
        ['#!/bin/sh \n' ...
        '# \n' ...
        '# Your job name \n' ...
        '#$ -N ',run_name,' \n' ...
        '# \n' ...
        '# Use current working directory \n' ...
        '#$ -cwd \n' ...
        '# \n' ...
        '# Join stdout and stderr \n' ...
        '#$ -j y \n' ...
        '# \n' ...
        '# Exclude specific node \n' ...
        '#$ -l h=''!ardbeg3.atmos.ucla.edu'' \n' ...
        '# \n' ...
        '# Select queue to run on \n' ...
        '#$ -q Run.q \n' ...
        '# \n' ...
        '# pe request for MPI. Set your number of processors here. \n' ...
        '# Make sure you use the "impi" parallel environment. \n' ...
        '#$ -pe impi 1 \n' ...
        '# \n' ...
        '# Run job through bash shell \n' ...
        '#$ -S /bin/bash \n' ...
        '# \n' ...
        '## Output file \n' ...
        '#$ -o ./output.txt \n' ...
        '# \n' ...
        '\n'];

      case 'hoffman2.idre.ucla.edu'

        pbsstr = ...
        ['#!/bin/sh \n' ...
        '# Your job name \n' ...
        '#$ -N ',run_name,' \n' ...
        '# \n' ...
        '# Use current working directory \n' ...
        '#$ -cwd \n' ...
        '# \n' ...
        '# Join stdout and stderr \n' ...
        '#$ -j y \n' ...
        '# \n' ...
        '# \n' ...
        '# pe request for MPI. Set your number of processors here. \n' ...
        '#$ -pe dc* 1 \n' ...
        '# \n' ...
        '# Run job through bash shell \n' ...
        '#$ -S /bin/bash \n' ...
        '# \n' ...
        '## Output file \n' ...
        '#$ -o ./output.txt \n' ...
        '# \n' ...
        '# The following is for reporting only. It is not really needed \n' ...
        '# to run the job. It will show up in your output file. \n' ...
        '# echo "Got $num_proc processors." \n' ...
        '# echo "Machines:" \n' ...
        '# cat $TMPDIR/machines \n' ...
        '# \n' ...
        '# Use full pathname to make sure we are using the right mpirun \n' ...
        '# Gridengine will set NSLOTS and TMPDIR for you \n' ...        
        '#$ -m bea \n' ...
        '#$ -l h_data=1G,h_rt=336:00:00,highp \n'];

    end
  
    fprintf(pbsfid,pbsstr);
    fprintf(pbsfid,run_str);
    fclose(pbsfid);
    
    fprintf(sfid,'qsub run_pbs');   

  else
    
    fprintf(sfid,run_str);  
    
  end

  fclose(sfid);  
  
  %%% Create a file to clean the run folder of output files
  cleanstr = ['for f in ./*n=*.dat; do rm $f; done'];
  cleanfid = fopen(cleanfname,'w');
  if (cleanfid == -1)
    error(['Could not open ',cleanfname]);
  end
  fprintf(cleanfid,cleanstr);
  fclose(cleanfid);
  
  if (use_cluster)
    
    %%% Create a file to upload the run to the cluster    
    upstr = ['rsync -av --update ',fullfile('..',run_name),' ',uname,'@',cluster_addr,':',cluster_home_dir];
    upfid = fopen(upfname,'w');
    if (upfid == -1)
      error(['Could not open ',upfname]);
    end
    fprintf(upfid,'%s',upstr);
    fclose(upfid);
    
    %%% Create a file to download the run from the cluster
    downstr = ['rsync -av --update ',uname,'@',cluster_addr,':',cluster_home_dir,'/',run_name,' ../'];
    downfid = fopen(downfname,'w');
    if (downfid == -1)
      error(['Could not open ',downfname]);
    end
    fprintf(downfid,'%s',downstr);
    fclose(downfid);
    
  end

end


